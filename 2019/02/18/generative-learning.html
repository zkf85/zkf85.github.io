<!DOCTYPE html>
<html>
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>
      
      [CS229] Lecture 5 Notes - Descriminative Learning v.s. Generative Learning Algorithm
      
      2019
    </title>
    <link rel="shortcut icon" type="image/x-icon" href="/assets/res/favicon.ico">
	<!--
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/materialize/0.99.0/css/materialize.min.css">
	-->
    <link rel="stylesheet" href="/assets/css/materialize.min.css">
    <link rel="stylesheet" href="/assets/css/Material+Icons.css">
    <link rel="stylesheet" href="/assets/css/main.css">
    
    
    <link rel="stylesheet" href="/assets/css/post.css">
    
    
    
    <link rel="stylesheet" href="/assets/css/syntax.css">
    <link rel="alternate" type="application/rss+xml" title="RSS" href="/feed.xml">
    <link rel="sitemap" type="application/xml" title="Sitemap" href="/sitemap.xml">
    <!-- Begin Jekyll SEO tag v2.5.0 -->
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="[CS229] Lecture 5 Notes - Descriminative Learning v.s. Generative Learning Algorithm" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Keep Updating: 2019-02-18 Merge to Lecture #5 Note 2019-01-23 Add Part 2, Gausian discriminant analysis 2019-01-22 Add Part 1, A Review of Generative Learning Algorithms." />
<meta property="og:description" content="Keep Updating: 2019-02-18 Merge to Lecture #5 Note 2019-01-23 Add Part 2, Gausian discriminant analysis 2019-01-22 Add Part 1, A Review of Generative Learning Algorithms." />
<link rel="canonical" href="http://localhost:4000/2019/02/18/generative-learning" />
<meta property="og:url" content="http://localhost:4000/2019/02/18/generative-learning" />
<meta property="og:site_name" content="Everest" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-02-18T14:30:00+08:00" />
<script type="application/ld+json">
{"description":"Keep Updating: 2019-02-18 Merge to Lecture #5 Note 2019-01-23 Add Part 2, Gausian discriminant analysis 2019-01-22 Add Part 1, A Review of Generative Learning Algorithms.","@type":"BlogPosting","url":"http://localhost:4000/2019/02/18/generative-learning","headline":"[CS229] Lecture 5 Notes - Descriminative Learning v.s. Generative Learning Algorithm","dateModified":"2019-02-18T14:30:00+08:00","datePublished":"2019-02-18T14:30:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2019/02/18/generative-learning"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  </head>
  <body>
    <header>
	  <!--
      <nav class="top-nav teal">
	  -->
      <nav class="top-nav ohio-state-scarlet">
        <div class="nav-wrapper">
          <div class="container">
            <a class="page-title" href="/">Everest<sup>2019</sup></a>
            <img class="buckeye_leaf" src="/assets/res/buckeye_leaf.png">
          </div>
        </div>
      </nav>
      <div class="container">
        <a href="#" data-activates="slide-out" class="button-collapse top-nav full hide-on-large-only">
          <i class="material-icons">menu</i>
        </a>
      </div>
      <ul id="slide-out" class="side-nav fixed">
        <li>
          <div class="userView">
            <div class="background"></div>
            <a href="https://github.com/zkf85" target="_blank"><img class="circle z-depth-2" src="/assets/res/lion_king.jpg"></a>
            <span class="white-text name">Zhu, Kefeng</span>
			<!--
            <span class="white-text email">zkf1985@gmail.com</span>
			-->
            <span class="white-text email"><i>A Programmer, An AI Practitioner</i></span>
          </div>
        </li>

        <li class="subheader">BLOG</li>
        <li><a class="waves-effect" href="/"><i class="material-icons">home</i>Home</a></li>
        <li><a class="waves-effect" href="/categories"><i class="material-icons">sort</i>Categories</a></li>
        <li><a class="waves-effect" href="/tags"><i class="material-icons">label</i>Tags</a></li>
<!--
<li><a class="waves-effect" href="/feed.xml" target="_blank"><i class="material-icons">rss_feed</i>RSS</a></li>
-->        
        <li class="subheader">COLLECTIONS</li>
        <li><a class="waves-effect" href="/cs229"><i class="material-icons">widgets</i>CS229</a></li>
        <li><a class="waves-effect" href="/mit6006"><i class="material-icons">storage</i>MIT6006</a></li>
        <li><a class="waves-effect" href="/leetcode"><i class="material-icons">code</i>LeetCode</a></li>

        <li class="subheader">OTHERS</li>
        <li><a class="waves-effect" href="/about"><i class="material-icons">person</i>About Me</a></li>
        <!-- <li><a class="waves-effect" href="/contact"><i class="material-icons">email</i>Contact</a></li> -->
      </ul>
    </header>
    <main>

<!--
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
-->
<!-- KF 02/14/2019
The following snippet is to allow single $ style inline mode.
Remember that after enabling this, you have to enter the real dollar sings with `\$`
-->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<div class="container">
  <div id="post-info">
  <h1>[CS229] Lecture 5 Notes - Descriminative Learning v.s. Generative Learning Algorithm</h1>
  <ul class="collapsible hoverable" data-collapsible="accordion">
    <li>
      <div class="collapsible-header">
        <span>
          <i class="material-icons tooltipped" data-position="left" data-delay="30" data-tooltip="Date">date_range</i>
          02/18/2019 14:30
          <i id="indicate" class="right material-icons tooltipped" data-position="left" data-delay="30" data-tooltip="Show extra info">info</i>
        </span>
      </div>
      <div class="collapsible-body">
        <span>
          <i class="material-icons tooltipped" data-position="left" data-delay="30" data-tooltip="Categories">sort</i>
          
          
          
          <a href="/categories#notes_cap" target="_blank"><div class="chip">Notes</div></a>
          
        </span>
        <span>
          <i class="material-icons tooltipped" data-position="left" data-delay="30" data-tooltip="Tags">label</i>
          
          
          
          <a href="/tags#machine-learning" target="_blank"><div class="chip">machine learning</div></a>
          
          
          
          <a href="/tags#cs229" target="_blank"><div class="chip">cs229</div></a>
          
        </span>
      </div>
    </li>
  </ul>
</div>
<div class="row">
  <div class="col s12">
    <blockquote>
  <p>Keep Updating:</p>
  <ul>
    <li>2019-02-18 Merge to Lecture #5 Note</li>
    <li>2019-01-23 Add Part 2, Gausian discriminant analysis</li>
    <li>2019-01-22 Add Part 1, A Review of Generative Learning Algorithms.</li>
  </ul>
</blockquote>

<p><img src="/public/img/20190122-bayes-theorem.jpg" alt="" /></p>

<!--more-->

<ul id="markdown-toc">
  <li><a href="#1-basics-of-generative-learning-algorithms" id="markdown-toc-1-basics-of-generative-learning-algorithms">1. Basics of Generative Learning Algorithms</a>    <ul>
      <li><a href="#11-an-example-to-explain-the-initiative-ideas" id="markdown-toc-11-an-example-to-explain-the-initiative-ideas">1.1. An Example to Explain the Initiative Ideas:</a>        <ul>
          <li><a href="#approach-i" id="markdown-toc-approach-i"><strong>Approach I:</strong></a></li>
          <li><a href="#approach-ii" id="markdown-toc-approach-ii"><strong>Approach II:</strong></a></li>
        </ul>
      </li>
      <li><a href="#12-definitions" id="markdown-toc-12-definitions">1.2. Definitions:</a>        <ul>
          <li><a href="#discriminative-learning-algorithms" id="markdown-toc-discriminative-learning-algorithms"><strong>Discriminative</strong> learning algorithms:</a></li>
          <li><a href="#generative-learning-algorithms" id="markdown-toc-generative-learning-algorithms"><strong>Generative</strong> learning algorithms:</a></li>
        </ul>
      </li>
      <li><a href="#13-more-about-generative-learning-algorithms" id="markdown-toc-13-more-about-generative-learning-algorithms">1.3. More about Generative Learning Algorithms:</a></li>
    </ul>
  </li>
  <li><a href="#2-gaussian-discriminant-analysis-gda" id="markdown-toc-2-gaussian-discriminant-analysis-gda">2. Gaussian Discriminant Analysis (GDA)</a>    <ul>
      <li><a href="#21-the-multivariate-normal-distribution" id="markdown-toc-21-the-multivariate-normal-distribution">2.1. The multivariate normal distribution</a></li>
      <li><a href="#22-the-gaussian-discriminant-analysis-model" id="markdown-toc-22-the-gaussian-discriminant-analysis-model">2.2. The Gaussian Discriminant Analysis model</a>        <ul>
          <li><a href="#to-do-add-the-derivation" id="markdown-toc-to-do-add-the-derivation"><em>(TO DO: Add the derivation)</em></a></li>
          <li><a href="#predict" id="markdown-toc-predict">Predict:</a></li>
        </ul>
      </li>
      <li><a href="#23-gda-and-logistic-regression" id="markdown-toc-23-gda-and-logistic-regression">2.3. GDA and logistic regression</a>        <ul>
          <li><a href="#assumption-relations" id="markdown-toc-assumption-relations">Assumption relations:</a></li>
          <li><a href="#which-is-better" id="markdown-toc-which-is-better">Which is better?</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#3-naive-bayes" id="markdown-toc-3-naive-bayes">3. Naive Bayes</a>    <ul>
      <li><a href="#naive-bayes-nb-assumption" id="markdown-toc-naive-bayes-nb-assumption">Naive Bayes (NB) assumption:</a></li>
      <li><a href="#predict-1" id="markdown-toc-predict-1">Predict:</a></li>
      <li><a href="#laplace-smoothing" id="markdown-toc-laplace-smoothing">Laplace smoothing</a></li>
    </ul>
  </li>
  <li><a href="#reference" id="markdown-toc-reference">Reference</a></li>
</ul>

<h2 id="1-basics-of-generative-learning-algorithms">1. Basics of Generative Learning Algorithms</h2>

<p><em>(01/22/2019)</em></p>

<h3 id="11-an-example-to-explain-the-initiative-ideas">1.1. An Example to Explain the Initiative Ideas:</h3>
<p>Consider a classification problem in which we want to learn to distinguish between cats \((y=0)\) and dogs \((y=1)\):</p>

<h4 id="approach-i"><strong>Approach I:</strong></h4>

<p>Based on some features of an animal, given a training set, an algorithm like <strong><em>logistic regression</em></strong> or <strong><em>perceptron</em></strong> algorithm, tries to find a <strong>decision boundary</strong> (e.g. a straight line) to separate the cats and the dogs.</p>

<p><strong>To classify a new animal:</strong></p>

<p>Just check on which side of the decision boundary it falls.</p>

<h4 id="approach-ii"><strong>Approach II:</strong></h4>

<p>First, look at cats, build a model of <strong>what cats look like</strong>. Then, look at dogs, build another model of <strong>what dogs look like</strong>.</p>

<p><strong>To classify a new animal:</strong></p>

<p>Match the new animal against the cat model and the dog model respectively, to see whether the new animal looks more like the cats or more like the dogs we had seen in the training set.</p>

<h3 id="12-definitions">1.2. Definitions:</h3>
<h4 id="discriminative-learning-algorithms"><strong>Discriminative</strong> learning algorithms:</h4>

<p>Algorithms that try to <strong>learn \(p(y \vert x)\) directly</strong> (such as logistic regression) or algorithms that try to learn mappings directly from the space of input \(\chi\) to the labels \({0,1}\) (such as perceptron algorithm) are called <strong>discriminative</strong> learning algorithms.</p>

<h4 id="generative-learning-algorithms"><strong>Generative</strong> learning algorithms:</h4>

<p>Algorithms that instead try to model \(p(x \vert y)\) and \(p(y)\) are called <strong>generative</strong> learning algorithms.</p>

<h3 id="13-more-about-generative-learning-algorithms">1.3. More about Generative Learning Algorithms:</h3>

<p>Continue with the example of cats and dogs, if \(y\) indicates whether an sample is a cat (0) or a dog (1), then</p>
<ul>
  <li>\(p(x \vert y=0)\) models the distribution of cats’ features.</li>
  <li>\(p(x \vert y=1)\) models the distribution of dogs’ features.</li>
</ul>

<p>After modeling \(p(y)\) (called the <strong>class priors</strong>) and \(p(x \vert y)\), our algorithm can then use <strong>Bayes Rule</strong> to derive the posterior distribution on \(y\) given \(x\):</p>

<script type="math/tex; mode=display">p(y \vert x) = \frac{p(x \vert y)p(y)}{p(x)}.</script>

<p>Here, the denominator is given by \(p(x)=\sum_i p(x \vert y_i)p(y_i)\). In cats &amp; dogs example, \(p(x)=p(x \vert y=0)p(y=0)+p(x \vert y=1)p(y=1)\).</p>

<p>Actually, if we we’re calculating \(p(y \vert x)\) in order to make a prediction, then we don’t need to calculate the denominator, since</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\arg\underset{y}{\max} p(y \vert x) & = \arg \underset{y}{\max} \frac{p(x \vert y)p(y)}{p(x)} \\
& = \arg \underset{y}{\max} p(x \vert y)p(y).
\end{align} %]]></script>

<h2 id="2-gaussian-discriminant-analysis-gda">2. Gaussian Discriminant Analysis (GDA)</h2>

<p><em>(01/23/2019)</em></p>

<p>In this model (GDA), we assume that \(p(x \vert y)\) is distributed according to a <strong>multivariate normal distribution</strong>. First let’s talk briefly about the properties of multivariate normal distributions.</p>

<h3 id="21-the-multivariate-normal-distribution">2.1. The multivariate normal distribution</h3>
<p>The Definition of so-called <em>multivariate normal distribution</em>:</p>

<ul>
  <li>A distribution with a <strong>mean vector</strong> \(\mu \in \mathbb{R}^n\) and a <strong>covariance matrix</strong> \(\Sigma \in \mathbb{R}^{n \times n}\), where \(\Sigma \geq 0\) is symmetric and positive semi-definite. Also written “\(\mathcal{N}(\mu, \Sigma)\)”, it’s density is given by:</li>
</ul>

<script type="math/tex; mode=display">p(x; \mu, \Sigma) = \frac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}}\exp\left(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)\right).</script>

<p>where “\(\vert\Sigma\vert\)” denotes the determinant of the matrix \(\Sigma\).</p>

<ul>
  <li>For a random variable \(X\) distributed \(\mathcal{N}(\mu, \Sigma)\), the mean is given by \(\mu\):</li>
</ul>

<script type="math/tex; mode=display">\text{E}[X] = \int_x x\ p(x;\mu,\Sigma)dx = \mu</script>

<ul>
  <li>The <strong>covariance</strong> of a vector-valued random variable \(Z\) is defined as \(\text{Cov}(Z) = \text{E}[(Z-\text{E}[Z])(Z-\text{E}[Z])^T]\), which can also be written as \(\text{Cov}(Z) = \text{E}[ZZ^T]-(\text{E}[Z])(\text{E}[Z])^T\). If \(X \sim \mathcal{N}(\mu, \Sigma)\), then:</li>
</ul>

<script type="math/tex; mode=display">\text{Cov}(X) = \Sigma.</script>

<h3 id="22-the-gaussian-discriminant-analysis-model">2.2. The Gaussian Discriminant Analysis model</h3>
<p><em>(02/18/2019)</em></p>

<p>When $x$ are continuous-valued variables, we can use the so-called <em>Gaussian Discriminant Analysis (GDA)</em> which using <strong>multivariate normal distribution</strong> to model the different cases:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
y &\sim \mbox{Bernoulli}(\phi) \\
x \vert y=0 &\sim \mathcal{N}(\mu_0, \Sigma) \\
x \vert y=1 &\sim \mathcal{N}(\mu_1, \Sigma)
\end{align} %]]></script>

<p>which can be written as:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
p(y) &= \phi^y(1-\phi)^{1-y} \\
p(x\vert y=0) &= \frac{1}{(2\pi)^{n/2} \vert\Sigma\vert^{1/2}}\exp\left(-\frac{1}{2}(x-\mu_0)^T\Sigma^{-1}(x-\mu_0)\right) \\
p(x\vert y=1) &= \frac{1}{(2\pi)^{n/2} \vert\Sigma\vert^{1/2}}\exp\left(-\frac{1}{2}(x-\mu_1)^T\Sigma^{-1}(x-\mu_1)\right) 
\end{align} %]]></script>

<p>Now the parameters are: $\phi, \mu_0, \mu_1, \Sigma$. Apply maximum likelihood estimate, the log-likelihood would be:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
l(\phi, \mu_0,\mu_1,\Sigma) &= \log\prod_{i=1}^m p(x^{(i)}, y^{(i)}; \phi,\mu_0,\mu_1,\Sigma) \\
&= \log\prod_{i=1}^m p(x^{(i)} \vert y^{(i)};\mu_0,\mu_1,\Sigma)p(y^{(i)};\phi)
\end{align} %]]></script>

<h4 id="to-do-add-the-derivation"><em>(TO DO: Add the derivation)</em></h4>
<p>We get the maximum likelihood estimate of these parameters:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\phi &= \frac{1}{m} \sum_{i=1}^m 1\{ y^{(i)}=1 \} \\
\mu_0 &= \frac{\sum_{i=1}^m 1\{ y^{(i)}=0 \} x^{(i)}}{\sum_{i=1}m 1\{ y{(i)}=0 \}} \\
\mu_1 &= \frac{\sum_{i=1}^m 1\{ y^{(i)}=1 \} x^{(i)}}{\sum_{i=1}m 1\{ y{(i)}=1 \}} \\
\Sigma &= \frac{1}{m}\sum_{i=1}^m (x^{(i)} - \mu_{y^{(i)}})(x^{(i)} - \mu_{y^{(i)}})^T.
\end{align} %]]></script>

<h4 id="predict">Predict:</h4>
<p>Simply apply the rules mentioned above, with these estimates:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\arg\underset{y}{\max} p(y \vert x) & = \arg \underset{y}{\max} \frac{p(x \vert y)p(y)}{p(x)} \\
& = \arg \underset{y}{\max} p(x \vert y)p(y).
\end{align} %]]></script>

<h3 id="23-gda-and-logistic-regression">2.3. GDA and logistic regression</h3>
<p>With the predicting rules above, we have:</p>

<script type="math/tex; mode=display">p(y=1 vert x;\phi,\Sigma,\mu_0,\mu_1) = \frac{1}{1 + \exp(-\theta^Tx)}.</script>

<p>where $\theta$ is some appropriate function of $\phi,\Sigma,\mu_0,\mu_1$. This is exactly the form that logistic regression, a discriminative algorithm, to model $p(y=1 \vert x)$.</p>

<h4 id="assumption-relations">Assumption relations:</h4>
<p><script type="math/tex">% <![CDATA[
\begin{align}
p(x \vert y) \text{ is multivariate gaussian} &\Rightarrow p(x \vert y) \text{ follows a logistic function/ExpFamily} \\
p(x \vert y) \text{ is multivariate gaussian} &\nLeftarrow p(x \vert y) \text{ follows a logistic function/ExpFamily} \\
\end{align} %]]></script></p>

<h4 id="which-is-better">Which is better?</h4>
<ul>
  <li>When $p(x \vert y)$ is really Gaussian, GDA is <strong>asymptotically efficient!</strong></li>
  <li>By making <strong>weaker assumptions</strong> logistic regression is more <em>robust</em>, especially the data is indeed non-Gaussian, then in the limit of large datasets, logistic regression will almost always do better than GDA. For this reason, in practice logistic regression is used more often than GDA.</li>
</ul>

<h2 id="3-naive-bayes">3. Naive Bayes</h2>
<p><em>(02/18/2019)</em></p>

<p>As GDA is for continuous $x$ problems, here Naive Bayes is for those problems with discrete $x$.</p>

<p>Here’s the typical example: spam filter.</p>

<p>First, we present the email(text) via a feature vector whose length is the size of word dictionary (a fixed number). If the email contains the $j$-th word, $x_j=1$; otherwise $x_j=0$. The vector would like like this:</p>

<script type="math/tex; mode=display">\begin{align}
x = \left[\begin{array}{c} 1\\0\\0\\\vdots\\1\\\vdots\\0 \end{array}\right] \quad \begin{array}{l} \text{a}\\\text{aardvark}\\\text{aardwolf}\\\vdots\\\text{buy}\\\vdots\\\text{zygmurgy}\end{array} \\
\end{align}</script>

<p>If our vocabulary size is 50000, then $x \in \{ 0, 1 \}^{50000}$ ($x$ is a 50000-dimensional vector of 0’s and 1’s). If we were to model $x$ discriminatively/explicitly with multinomial distribution over the $2^{50000}$ possible outcomes, the we’d end up with a ($2^{50000} -1$)-dimensional parameter vector, which is not appliable.</p>

<h4 id="naive-bayes-nb-assumption">Naive Bayes (NB) assumption:</h4>
<ul>
  <li>To model $p(x \vert y)$, we have to assume that the $x_i$’s are <strong>conditionally independent</strong> given $y$.</li>
  <li>Under such assumption, the resulting algorithm is called the <strong>Naive Bayes classifier</strong>.</li>
</ul>

<p>Then, we have:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
& p(x_1,\dots,x_{50000} \vert y) \\
&= p(x_1 \vert y)p(x_2 \vert y, x_1)p(x_3 \vert y, x_1,x_2)\cdots p(x_{50000}\vert y,x_1,\dots,x_{49999}) \\
&= p(x_1\vert y)p(x_2\vert y)p(x_3\vert y)\cdots p(x_{50000}\vert y) \\
&= \prod_{j=1}^n p(x_j\vert y)
\end{align} %]]></script>

<p>Our model now is parameterized by:</p>

<script type="math/tex; mode=display">% <![CDATA[
\left\{ 
\begin{array}{rcl} 
\phi_{j\vert y=1} &= &p(x_j=1\vert y=1) \\
\phi_{j\vert y=0} &= &p(x_j=1\vert y=0) \\
\phi_y &= &p(y=1)
\end{array}
\right. %]]></script>

<p>The likelihood of the data:</p>

<script type="math/tex; mode=display">\mathcal{L}(\phi_y,\phi_{j\vert y=0},\phi_{j\vert y=1}) = \prod_{i=1}^m p(x^{(i)}, y^{(i)}).</script>

<p>Then we can get the maximum likelihood estimates:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\hat\phi_{j\vert y=1} &= \frac{\sum_{i=1}^m 1\{x_j^{(i)} =1 \wedge y^{(i)} = 1 \}}{\sum_{i=1}^m 1\{y^{(i)} = 1 \}} \\
\hat\phi_{j\vert y=0} &= \frac{\sum_{i=1}^m 1\{x_j^{(i)} =1 \wedge y^{(i)} = 0 \}}{\sum_{i=1}^m 1\{y^{(i)} = 0 \}} \\
\hat\phi_y &= \frac{\sum_{i=1}^m 1\{y^{(i)} = 1\}}m
\end{align} %]]></script>

<p>In the equation above, the “$\wedge$” symbol means “and”. The estimates have a very natural interpretation. For instance, $\phi_{j\vert y=1}$ is just the fraction of spam $(y=1)$ emails in which word $j$ does appear.</p>

<h4 id="predict-1">Predict:</h4>
<p>To make a prediction on a new sample with feature $x$, we then simply calculate:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
p(y=1\vert x) &= \frac{p(x\vert y=1)p(y=1)}{p(x)} \\
&= \frac{\left(\prod_{j=1}^n p(x_j\vert y=1)\right)p(y=1)}{\prod_{j=1}^n p(x_j\vert y=1)p(y=1) + \prod_{j=1}^n p(x_j\vert y=0)p(y=0)}
\end{align} %]]></script>

<p>and pick the class has the higher posterior probability.</p>

<h3 id="laplace-smoothing">Laplace smoothing</h3>
<p>Here is a problem:</p>
<ul>
  <li>If there is a new word ($k$-th in the dictionary) appeared in the email text, which is not included in you training set. Then,</li>
</ul>

<script type="math/tex; mode=display">\prod_{j=1}^n p(x_j\vert y=1) = 0\Leftarrow p(x_k\vert y=1) = 0\\
\prod_{j=1}^n p(x_j\vert y=0) = 0 \Leftarrow p(x_k\vert y=1) = 0</script>

<p>Then,</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
p(y=1\vert x) &= \frac{\left(\prod_{j=1}^n p(x_j\vert y=1)\right)p(y=1)}{\prod_{j=1}^n p(x_j\vert y=1)p(y=1) + \prod_{j=1}^n p(x_j\vert y=0)p(y=0)} \\
&= \frac0{0}.
\end{align} %]]></script>

<p>To avoid this, we introduce <strong>Laplace smoothing</strong>, which replace the estimate with:</p>

<script type="math/tex; mode=display">\phi_j = \frac{\sum_{i=1}^m 1 \{z^{(i)} \} + 1}{m+k}.</script>

<p>Then the estimates is updated as:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\hat\phi_{j\vert y=1} &= \frac{\sum_{i=1}^m 1\{x_j^{(i)} =1 \wedge y^{(i)} = 1 \} + 1 }{\sum_{i=1}^m 1\{y^{(i)} = 1 \} + 2 } \\
\hat\phi_{j\vert y=0} &= \frac{\sum_{i=1}^m 1\{x_j^{(i)} =1 \wedge y^{(i)} = 0 \} + 1 }{\sum_{i=1}^m 1\{y^{(i)} = 0 \} + 2 } 
\end{align} %]]></script>

<blockquote>
  <p><strong>Note:</strong>
In practice, it usually doesn’t matter much whether we apply Laplace smoothing to $\phi_y$ or not, since we will typically have a fair fraction each of spam and non-spam message, so $\phi_y$ will be a reasonable estimate of $p(y=1)$ and will be quite far from 0 anyway. <span style="color:crimson"> WHY? </span></p>
</blockquote>

<h2 id="reference">Reference</h2>
<ol>
  <li><a href="https://see.stanford.edu/materials/aimlcs229/cs229-notes2.pdf">» Stanford CS229 Lecture Note Part IV - Generative Learning Algorithms</a></li>
</ol>

<p><br /><br /><strong><em>KF</em></strong></p>

  </div>
</div>


<div>
<h2>Comments</h2>
<div id="disqus_thread"></div>
<script>
  var disqus_config = function () {
    this.page.url = "http://localhost:4000/2019/02/18/generative-learning";
    this.page.identifier = "/2019/02/18/generative-learning";
  };
  (function() { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
    s.src = '//zkf85githubio.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
  })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>



</div>

<!-- KF 02/13/2019  -->
<!-- Add back to top button -->
<button onclick="topFunction()" id="myBtn" title="Back to top"><i class="material-icons">vertical_align_top</i></button>

<script>
// When the user scrolls down 20px from the top of the document, show the button
window.onscroll = function() {scrollFunction()};

function scrollFunction() {
  if (document.body.scrollTop > 20 || document.documentElement.scrollTop > 20) {
    document.getElementById("myBtn").style.display = "block";
  } else {
    document.getElementById("myBtn").style.display = "none";
  }
}

// When the user clicks on the button, scroll to the top of the document
function topFunction() {
  document.body.scrollTop = 0;
  document.documentElement.scrollTop = 0;
}
</script>
    </main>
    <footer class="page-footer ohio-state-scarlet">
      <div class="container">
        <div class="row">
          <div class="col s12">
            <img src="/assets/res/logo.png" alt="logo"/>
            <p class="grey-text text-lighten-4">Theme based on <a href="http://materializecss.com">Materialize.css</a> for jekyll sites.
</p>
          </div>
        </div>
      </div>
      <div class="footer-copyright">
        <div class="container">
          &#xA9; 2019 Everest. All rights reserved. Powered by <a href="https://github.com/zkf85">Kefeng</a>.
        </div>
      </div>
    </footer>
    <script src="//code.jquery.com/jquery-2.2.4.min.js" integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44=" crossorigin="anonymous"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/materialize/0.99.0/js/materialize.min.js"></script>
    
    
    <script src="/assets/js/post.js"></script>
    
    
    
    
    <script src="/assets/js/main.js"></script>
  </body>
</html>


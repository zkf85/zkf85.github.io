<blockquote>
  <p><em>“Deep Learning”</em> Reading note</p>
</blockquote>

<!--more-->

<h3 id="10-sequence-modeling-recurrent-and-recursive-nets">10. Sequence Modeling: Recurrent and Recursive nets</h3>

<h4 id="101-unfolding-computational-graphs">10.1 Unfolding Computational Graphs</h4>

<ul>
  <li>
    <p><strong>What is a Computational Graph?</strong></p>

    <p>A computational graph is a way to formalize the structure of a set of computations.</p>
  </li>
</ul>

<p>Considering a classical form of a dynamic system:</p>

<script type="math/tex; mode=display">s^{(t)} = f \left( s^{(t-1)} ; \theta \right), \tag{10.1}</script>

<p>where $s$ is called the state of the system. And it is a recurrent.</p>

<p>For a finite number of $\tau$ steps, we can unfold the equation. For example, when $\tau = 3$, we have</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{eqnarray*}
s^{(3)} &=& f\left( s^{(2)}; \theta \right) \tag{10.2} \\
 &=& f\left(f\left( s^{(1)}; \theta \right);\theta\right) \tag{10.3}
 \end{eqnarray*} %]]></script>

<p>Another example, considering a dynamic system driven by an external signal $x^{(t)}$,
<script type="math/tex">s^{(t)} = f\left( s^{(t-1)}, x^{(t)};\theta \right) \tag{10.4}</script></p>

<p>To indicate the state  is the hidden units of the network, we rewrite equation 10.4 using $h$ to represent the state,
<script type="math/tex">h^{(t)} = f\left(h^{(t-1)}, x^{(t)}; \theta \right) \tag{10.5}</script></p>

<p>We can represent the unfolded recurrence after $t$ steps with a function $g^{(t)}$:
<script type="math/tex">% <![CDATA[
\begin{eqnarray*}
h^{(t)} &=& g^{(t)}\left(x^{(t)}, x^{(t-1)}, x^{(t-2)}, ..., x^{(2)}, x^{(1)}\right) \tag{10.6} \\
&=& f\left(h^{(t-1)}, x^{(t)};\theta\right). \tag{10.7}
\end{eqnarray*} %]]></script></p>

<p>The function $ g^{(t)} $ takes the whole past sequence $\left(x^{(t)}, x^{(t-1)}, x^{(t-2)}, …, x^{(2)}, x^{(1)} \right)$ as input and produces the current state. But the unfolded recurrent structure allows us to factorize the function into repeated application of a function $f$, the unfolding process introduces two major advantages:</p>

<ul>
  <li>
    <ol>
      <li>The learned model always has the same input size, which specified in terms of transition from one state to another.</li>
    </ol>
  </li>
  <li>
    <ol>
      <li>It is possible to use the <em>same</em> transition function $f$ with the same parameters at every time step.</li>
    </ol>
  </li>
</ul>

<h4 id="102-recurrent-neural-networks-循环神经网络">10.2 Recurrent Neural Networks （循环神经网络）</h4>
<blockquote>
  <p>Based on the basic idea of parameter-sharing, we can build a variety of neural networks. 3 typical designs are presented in the book:</p>
</blockquote>

<ul>
  <li>
    <ol>
      <li>RNN produce an output at each time step, and have connections between hidden unit. (Page 368, fig. 10.3)</li>
    </ol>
  </li>
  <li>
    <ol>
      <li>RNN produce an output at each time step and have connections from <em>the output at one time step</em> to <em>the hidden units at the next time step</em>. (Page 370, fig. 10.4)</li>
    </ol>
  </li>
  <li>
    <ol>
      <li>RNN connections between hidden units, that read an entire sequence and then produce a single output (page. 371, fig. 10.5)</li>
    </ol>
  </li>
</ul>

<blockquote>
  <p>The RNN of figure 10.3 (p.368) and equation 10.8 (p.369) is universal in the sense that (从…意义上来说）<em>any</em>function computable by a Turing machine can be computed by such a recurrent network of a finite size.</p>
</blockquote>

<p>In the book, case 1 is chosen as a throughout example. 
Figure 10.3 (p.369) or say equation 10.8 (p. 370) defines forward propagation in this model.</p>

<script type="math/tex; mode=display">% <![CDATA[
\DeclareMathOperator{\softmax}{softmax}
\begin{eqnarray*}
\pmb{a}^{(t)} &=& \pmb{b} + \pmb{W}\pmb{h}^{(t-1)} + \pmb{U}\pmb{x}, \tag{10.8} \\
\pmb{h}^{(t)} &=& \tanh\left(\pmb{a}^{(t)}\right), \tag{10.9} \\
\pmb{o}^{(t)} &=& \pmb{c} + \pmb{V}\pmb{h}^{(t)}, \tag{10.10} \\
\hat{\pmb{y}} ^{(t)} &=& \softmax \left(\pmb{o}^{(t)} \right), \tag{10.11}
\end{eqnarray*} %]]></script>

<p>Parameters: bias vecters $\pmb{b}$, $\pmb{c}$, weight matrices $\pmb{U}$, $\pmb{V}$ and $\pmb{W}$.</p>

<ul>
  <li>$\pmb W$ is the hidden-to-hidden recurrent weight matrix</li>
  <li>$\pmb V$ is the hidden-to-output weight matrix</li>
  <li>
    <p>$\pmb U$ is the input-to-hidden weight matrix</p>
  </li>
  <li>This is an example of a recurrent network that maps an input sequence $\pmb{x}^{(t)}$ to an output sequence $\pmb{y}^{(t)}$ of the same length $t$ in a range of $(1, \tau)$.</li>
</ul>

<blockquote>
  <p>The Loss Function:</p>
</blockquote>

<p><br /><strong><em>KF</em></strong></p>

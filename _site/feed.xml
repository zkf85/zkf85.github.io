<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2019-02-12T16:28:56+08:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">ZKF’s Playground</title><subtitle>Theme based on &lt;a href=&quot;http://materializecss.com&quot;&gt;Materialize.css&lt;/a&gt; for jekyll sites.
</subtitle><entry><title type="html">[CS229] Lecture 3 Notes</title><link href="http://localhost:4000/2019/01/31/cs229-lecture-3" rel="alternate" type="text/html" title="[CS229] Lecture 3 Notes" /><published>2019-01-31T10:40:00+08:00</published><updated>2019-01-31T10:40:00+08:00</updated><id>http://localhost:4000/2019/01/31/cs229-lecture-3</id><content type="html" xml:base="http://localhost:4000/2019/01/31/cs229-lecture-3">&lt;blockquote&gt;
  &lt;p&gt;CS229 Lecture #3:&lt;/p&gt;
  &lt;ol&gt;
    &lt;li&gt;Locally Weighted Regression&lt;/li&gt;
    &lt;li&gt;Probablistic Interpretation of LR&lt;/li&gt;
    &lt;li&gt;Classification (Logistic Regression)&lt;/li&gt;
    &lt;li&gt;Disgression -&amp;gt; Perceptron&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;!--more--&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#1-locally-weighted-regression&quot; id=&quot;markdown-toc-1-locally-weighted-regression&quot;&gt;1. Locally Weighted Regression&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#11-the-origin&quot; id=&quot;markdown-toc-11-the-origin&quot;&gt;1.1. The origin:&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#12-detail&quot; id=&quot;markdown-toc-12-detail&quot;&gt;1.2. Detail:&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#13-parametric-vs-non-parametric-learning-algorighms&quot; id=&quot;markdown-toc-13-parametric-vs-non-parametric-learning-algorighms&quot;&gt;1.3. Parametric v.s. Non-Parametric Learning Algorighms:&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#parametric-learning-algorithm&quot; id=&quot;markdown-toc-parametric-learning-algorithm&quot;&gt;“Parametric” learning algorithm:&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#non-parametric-learning-algorithm&quot; id=&quot;markdown-toc-non-parametric-learning-algorithm&quot;&gt;“Non-Parametric” learning algorithm:&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#2-probablistic-interpretation-of-lr&quot; id=&quot;markdown-toc-2-probablistic-interpretation-of-lr&quot;&gt;2. Probablistic Interpretation of LR&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#21-assumptions&quot; id=&quot;markdown-toc-21-assumptions&quot;&gt;2.1. Assumptions:&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#22-maximum-likelihood&quot; id=&quot;markdown-toc-22-maximum-likelihood&quot;&gt;2.2. Maximum Likelihood&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#221-there-are-two-different-basic-ideas-in-probability&quot; id=&quot;markdown-toc-221-there-are-two-different-basic-ideas-in-probability&quot;&gt;2.2.1. There are two different basic ideas in probability:&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#222-the-principle-of-maximum-likelihood&quot; id=&quot;markdown-toc-222-the-principle-of-maximum-likelihood&quot;&gt;2.2.2. The principle of maximum likelihood:&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#23-find-the-best-theta-to-reach-the-maximum-likelihood&quot; id=&quot;markdown-toc-23-find-the-best-theta-to-reach-the-maximum-likelihood&quot;&gt;2.3. Find the best \(\theta\) to reach the “Maximum Likelihood”:&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#3-classification-and-logistic-regression&quot; id=&quot;markdown-toc-3-classification-and-logistic-regression&quot;&gt;3. Classification and Logistic Regression&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#31-logistic-regression-for-classification-problems&quot; id=&quot;markdown-toc-31-logistic-regression-for-classification-problems&quot;&gt;3.1. Logistic regression for classification problems&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#4-digression---perceptron&quot; id=&quot;markdown-toc-4-digression---perceptron&quot;&gt;4. Digression - Perceptron&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#reference&quot; id=&quot;markdown-toc-reference&quot;&gt;Reference&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;1-locally-weighted-regression&quot;&gt;1. Locally Weighted Regression&lt;/h2&gt;
&lt;h3 id=&quot;11-the-origin&quot;&gt;1.1. The origin:&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The choice of features is important to ensuring good performance of a learning algorithm.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The Locally weighted linear regression (LWR) algorithm, which assuming there is sufficient training data, makes the &lt;strong&gt;choice of features less critical&lt;/strong&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;12-detail&quot;&gt;1.2. Detail:&lt;/h3&gt;
&lt;p&gt;The locally weighted linear regression alogorithm does the following:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Fit \( \theta \) to minimize \(\sum_i w^{(i)} (y^{(i)} - \theta^T x^{(i)} )^2\).&lt;/li&gt;
  &lt;li&gt;Output \(\theta^T x\).&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;where \(w^{(i)} = \exp\left(-\frac{(x^{(i)} - x)^2}{2\tau^2}\right)\), \(\tau\) is called &lt;strong&gt;“bandwidth”&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Note that:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;the wights depend on the particular point \(x\) at which we’re trying to evaluate \(x\).&lt;/li&gt;
  &lt;li&gt;if \(\vert x^{(i)} - x \vert\) is small, the \(w^{(i)}\) is close to 1;&lt;/li&gt;
  &lt;li&gt;if \(\vert x^{(i)} - x \vert\) is large, the \(w^{(i)}\) is close to 0;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;13-parametric-vs-non-parametric-learning-algorighms&quot;&gt;1.3. Parametric v.s. Non-Parametric Learning Algorighms:&lt;/h3&gt;
&lt;h4 id=&quot;parametric-learning-algorithm&quot;&gt;“Parametric” learning algorithm:&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;Fix numbers of parameters to fit the model;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;non-parametric-learning-algorithm&quot;&gt;“Non-Parametric” learning algorithm:&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;Number of parameter grows with \(m\) (\(m\) is the training size).&lt;/li&gt;
  &lt;li&gt;To make predictions using LWR, we need to keep the entire training set around. (LWR is the first example in the class as a &lt;strong&gt;non-parametric&lt;/strong&gt; algorithm).&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;2-probablistic-interpretation-of-lr&quot;&gt;2. Probablistic Interpretation of LR&lt;/h2&gt;
&lt;h3 id=&quot;21-assumptions&quot;&gt;2.1. Assumptions:&lt;/h3&gt;
&lt;p&gt;We &lt;strong&gt;assume&lt;/strong&gt; that:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y{(i)} = \theta^T x^{(i)} + \varepsilon^{(i)}&lt;/script&gt;

&lt;p&gt;where&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;\(\varepsilon^{(i)}\) is the error and \(\varepsilon^{(i)} \sim \mathcal{N}(0, \sigma^2)\), which means \(P(\varepsilon^{(i)}) = \frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{(y^{(i)} - \theta^T x^{(i)})^2}{2\sigma^2}\right)\).&lt;/li&gt;
  &lt;li&gt;\(\varepsilon^{(i)}\) is I.I.D. (independently identically distributed).&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;22-maximum-likelihood&quot;&gt;2.2. Maximum Likelihood&lt;/h3&gt;
&lt;h4 id=&quot;221-there-are-two-different-basic-ideas-in-probability&quot;&gt;2.2.1. There are two different basic ideas in probability:&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;Frequency point of view -&amp;gt; (Applied here!)&lt;/li&gt;
  &lt;li&gt;Bayesian point of view.&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;222-the-principle-of-maximum-likelihood&quot;&gt;2.2.2. The principle of maximum likelihood:&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;To choose \(\theta\) to maximum \(L(\theta)\), \(\theta\) is NOT a random value, but a true value out there!&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;23-find-the-best-theta-to-reach-the-maximum-likelihood&quot;&gt;2.3. Find the best \(\theta\) to reach the “Maximum Likelihood”:&lt;/h4&gt;
&lt;p&gt;Then, the “Likelihood” of the \(y\) is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
L(\theta) &amp; = P\left(\mathbf{y} \vert X;\theta\right) \\
(i.i.d) &amp; = \prod_{i=1}^m P\left(y^{(i)} \vert x^{(i)}; \theta\right) \\
&amp; = \prod_{i=1}^m \frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{\left(y^{(i)} - \theta^T x^{(i)}\right)^2}{2\sigma^2}\right)
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Then, for mathematical convenience, let&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
l(\theta) &amp;= \log\left(L(\theta)\right) \\
&amp;= \log\left(\prod_{i=1}^m P\left(y^{(i)} \vert x^{(i)}; \theta\right)\right) \\
&amp;= \sum_{i=1}^m \log P\left(y^{(i)} \vert x^{(i)}; \theta\right) \\
&amp;= \sum_{i=1}^m \log\left(\frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{\left(y^{(i)} - \theta^T x^{(i)}\right)^2}{2\sigma^2}\right)\right) \\
&amp;= m\cdot\log\frac{1}{\sqrt{2\pi}\sigma} + \sum_{i=1}^m \left(-\frac{\left(y^{(i)} - \theta^T x^{(i)}\right)^2}{2\sigma^2}\right).
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Therefore, to &lt;strong&gt;maximize&lt;/strong&gt; \(l(\theta)\) is to &lt;strong&gt;minimize&lt;/strong&gt; \(\sum_{i=1}^m \frac{\left(y^{(i)} - \theta^T x^{(i)}\right)^2}{2\sigma^2}\), which is the same as minimizing \(J(\theta) = \sum_{i=1}^m \frac{\left(y^{(i)} - \theta^T x^{(i)}\right)^2}{2}\) (this is the rule for Least Square).&lt;/p&gt;

&lt;h2 id=&quot;3-classification-and-logistic-regression&quot;&gt;3. Classification and Logistic Regression&lt;/h2&gt;
&lt;blockquote&gt;
  &lt;p&gt;Note: 
Generally, it’s bad idea to use LR for classification problems.
Why? It doesn’t make sence for \(h_\theta(x)\) to take values larger than 1 or smaller than 0 when we know that \(y\in{0,1}\).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;31-logistic-regression-for-classification-problems&quot;&gt;3.1. Logistic regression for classification problems&lt;/h3&gt;
&lt;p&gt;As in a classification problem:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y \in \{0, 1\} \\
h_\theta(x) \in [0, 1]&lt;/script&gt;

&lt;p&gt;To fix the problems when using Linear Regression, we choose another form of function for our hypothesis \(h_\theta(x)\).&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;h_\theta(x) = g(\theta^Tx) = \frac{1}{1 + e^{-\theta^T x}},&lt;/script&gt;

&lt;p&gt;where&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;g(z) = \frac{1}{1 + e^{-z}}.&lt;/script&gt;

&lt;p&gt;\(g(z) = \frac{1}{1 + e^{-z}}\) is called “sigmoid” function or “logistic” function.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/public/img/20190212_sigmoid_function.eps&quot; alt=&quot;&quot; /&gt;
Then, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\left\{ \begin{array}{rcl} 
P(y=1 \vert x; \theta) &amp; = &amp; h_\theta(x) \\
P(y=0 \vert x; \theta) &amp; = &amp; 1 - h_\theta(x)
\end{array}\right. \\
\Rightarrow P(y|x;\theta) = h_\theta(x)^y (1 - h_\theta(x))^{1-y} %]]&gt;&lt;/script&gt;

&lt;p&gt;Then the likelihood is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
L(\theta) &amp; = P\left(\mathbf{y} \vert X;\theta\right) \\
&amp; = \prod_{i=1}^m P\left(y^{(i)} \vert x^{(i)}; \theta\right)
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;For mathematical convenience, let&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
l(\theta) &amp;= \log\left(L(\theta)\right) \\
&amp;= \log\left(\prod_{i=1}^m P\left(y^{(i)} \vert x^{(i)}; \theta\right)\right) \\
&amp;= \sum_{i=1}^m \log P\left(y^{(i)} \vert x^{(i)}; \theta\right) \\
&amp;= \sum_{i=1}^m \left( y^{(i)} \log h_\theta(x^{(i)}) + (1 - y^{(i)}) \log (1 - h_\theta(x^{(i)})\right)
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;To maximize the likelihood, here we use &lt;strong&gt;gradient ascent&lt;/strong&gt; by \(\theta := \theta + \alpha \nabla_\theta l(\theta)\). The derivative is (derivation omitted, can be found on Page 18 in the notes):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial}{\partial\theta_j}l(\theta) = \sum_{i=1}^m \left(y^{(i)} - h_\theta\left(x^{(i)}\right)\right)\cdot x_j^{(i)}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta_j := \theta_j + \alpha \sum_{i=1}^m \left(y^{(i)} - h_\theta\left(x^{(i)}\right)\right)\cdot x_j^{(i)}&lt;/script&gt;

&lt;h2 id=&quot;4-digression---perceptron&quot;&gt;4. Digression - Perceptron&lt;/h2&gt;
&lt;p&gt;Almost the same procedure as the logistic regression. The only difference is the \(g(z)\) used in the process.&lt;/p&gt;

&lt;p&gt;The \(g(z)\) used in perceptron learning algorithm is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
g(z) = \left\{ \begin{array}{rcl} 
1 &amp; \mbox{if} &amp; z \ge 0 \\
0 &amp; \mbox{if} &amp; z \lt 0
\end{array}\right. %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;img src=&quot;/public/img/20190212_perceptron_function.eps&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Then, \(h_\theta(x) = g(\theta^T x)\), and the update rule for perceptron learning algorithm is almost the same:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta_j := \theta_j + \alpha \left(y^{(i)} - h_\theta\left(x^{(i)}\right)\right)\cdot x_j^{(i)}&lt;/script&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://see.stanford.edu/materials/aimlcs229/cs229-notes1.pdf&quot;&gt;» Stanford CS229 Lecture Note Part I &amp;amp; II&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;strong&gt;&lt;em&gt;KF&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;</content><author><name></name></author><category term="[&quot;Notes&quot;]" /><category term="cs229" /><category term="machine learning" /><summary type="html">CS229 Lecture #3: Locally Weighted Regression Probablistic Interpretation of LR Classification (Logistic Regression) Disgression -&amp;gt; Perceptron</summary></entry><entry><title type="html">Random! Shuffle v.s. Permutation (Numpy)</title><link href="http://localhost:4000/2019/01/31/np-shuffle-vs-permutation" rel="alternate" type="text/html" title="Random! Shuffle v.s. Permutation (Numpy)" /><published>2019-01-31T10:40:00+08:00</published><updated>2019-01-31T10:40:00+08:00</updated><id>http://localhost:4000/2019/01/31/np-shuffle-vs-permutation</id><content type="html" xml:base="http://localhost:4000/2019/01/31/np-shuffle-vs-permutation">&lt;blockquote&gt;
  &lt;p&gt;Generally, in Numpy, both &lt;code class=&quot;highlighter-rouge&quot;&gt;random.permutation&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;random.shuffle&lt;/code&gt; randomly shuffle elements in an array. But there are differences:&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;difference&quot;&gt;Difference:&lt;/h2&gt;
&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;np.random.permutation&lt;/code&gt; has two differences from &lt;code class=&quot;highlighter-rouge&quot;&gt;np.random.shuffle&lt;/code&gt;:&lt;/p&gt;

&lt;p&gt;if passed an array, it will return a shuffled &lt;strong&gt;copy&lt;/strong&gt; of the array; &lt;code class=&quot;highlighter-rouge&quot;&gt;np.random.shuffle&lt;/code&gt; shuffles the array &lt;strong&gt;inplace&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;if passed an integer, it will return a shuffled range i.e. &lt;code class=&quot;highlighter-rouge&quot;&gt;np.random.shuffle(np.arange(n))&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;If x is an integer, randomly permute &lt;code class=&quot;highlighter-rouge&quot;&gt;np.arange(x)&lt;/code&gt;. If x is an array, make a copy and shuffle the elements randomly.&lt;/p&gt;

&lt;p&gt;The source code might help to understand this:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;mi&quot;&gt;3280&lt;/span&gt;        &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;permutation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;object&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;3307&lt;/span&gt;            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;isinstance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;integer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;3308&lt;/span&gt;                &lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;3309&lt;/span&gt;            &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;3310&lt;/span&gt;                &lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;3311&lt;/span&gt;            &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shuffle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;3312&lt;/span&gt;            &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://stackoverflow.com/questions/15474159/shuffle-vs-permute-numpy&quot;&gt;» shuffle vs permute numpy - Stack Overflow&lt;/a&gt;&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;strong&gt;&lt;em&gt;KF&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;</content><author><name></name></author><category term="[&quot;Python&quot;]" /><category term="numpy" /><category term="random" /><summary type="html">Generally, in Numpy, both random.permutation and random.shuffle randomly shuffle elements in an array. But there are differences: Difference: np.random.permutation has two differences from np.random.shuffle: if passed an array, it will return a shuffled copy of the array; np.random.shuffle shuffles the array inplace if passed an integer, it will return a shuffled range i.e. np.random.shuffle(np.arange(n)) If x is an integer, randomly permute np.arange(x). If x is an array, make a copy and shuffle the elements randomly. The source code might help to understand this: 3280 def permutation(self, object x): ... 3307 if isinstance(x, (int, np.integer)): 3308 arr = np.arange(x) 3309 else: 3310 arr = np.array(x) 3311 self.shuffle(arr) 3312 return arr Reference » shuffle vs permute numpy - Stack Overflow</summary></entry><entry><title type="html">[CS229] Descriminative Learning v.s. Generative Learning Algorithm</title><link href="http://localhost:4000/2019/01/23/generative-learning" rel="alternate" type="text/html" title="[CS229] Descriminative Learning v.s. Generative Learning Algorithm" /><published>2019-01-23T16:30:00+08:00</published><updated>2019-01-23T16:30:00+08:00</updated><id>http://localhost:4000/2019/01/23/generative-learning</id><content type="html" xml:base="http://localhost:4000/2019/01/23/generative-learning">&lt;blockquote&gt;
  &lt;p&gt;A Review of Generative Learning Algorithms.&lt;/p&gt;

  &lt;p&gt;Keep Updating:&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;2019-01-22 Add Part 1&lt;/li&gt;
    &lt;li&gt;2019-01-23 Add Part 2, Gausian discriminant analysis&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/public/img/20190122-bayes-theorem.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;!--more--&gt;

&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#1-basics-of-generative-learning-algorithms&quot; id=&quot;markdown-toc-1-basics-of-generative-learning-algorithms&quot;&gt;1. Basics of Generative Learning Algorithms&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#11-an-example-to-explain-the-initiative-ideas&quot; id=&quot;markdown-toc-11-an-example-to-explain-the-initiative-ideas&quot;&gt;1.1. An Example to Explain the Initiative Ideas:&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#approach-i&quot; id=&quot;markdown-toc-approach-i&quot;&gt;&lt;strong&gt;Approach I:&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#approach-ii&quot; id=&quot;markdown-toc-approach-ii&quot;&gt;&lt;strong&gt;Approach II:&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#12-definitions&quot; id=&quot;markdown-toc-12-definitions&quot;&gt;1.2. Definitions:&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#discriminative-learning-algorithms&quot; id=&quot;markdown-toc-discriminative-learning-algorithms&quot;&gt;&lt;strong&gt;Discriminative&lt;/strong&gt; learning algorithms:&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#generative-learning-algorithms&quot; id=&quot;markdown-toc-generative-learning-algorithms&quot;&gt;&lt;strong&gt;Generative&lt;/strong&gt; learning algorithms:&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#13-more-about-generative-learning-algorithms&quot; id=&quot;markdown-toc-13-more-about-generative-learning-algorithms&quot;&gt;1.3. More about Generative Learning Algorithms:&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#2-gaussian-discriminant-analysis-gda&quot; id=&quot;markdown-toc-2-gaussian-discriminant-analysis-gda&quot;&gt;2. Gaussian Discriminant Analysis (GDA)&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#21-the-multivariate-normal-distribution&quot; id=&quot;markdown-toc-21-the-multivariate-normal-distribution&quot;&gt;2.1. The multivariate normal distribution&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#todo---practice&quot; id=&quot;markdown-toc-todo---practice&quot;&gt;//TODO - Practice:&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#22-the-gaussian-discriminant-analysis-model&quot; id=&quot;markdown-toc-22-the-gaussian-discriminant-analysis-model&quot;&gt;2.2. The Gaussian Discriminant Analysis model&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#reference&quot; id=&quot;markdown-toc-reference&quot;&gt;Reference&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;1-basics-of-generative-learning-algorithms&quot;&gt;1. Basics of Generative Learning Algorithms&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;(01/22/2019)&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;11-an-example-to-explain-the-initiative-ideas&quot;&gt;1.1. An Example to Explain the Initiative Ideas:&lt;/h3&gt;
&lt;p&gt;Consider a classification problem in which we want to learn to distinguish between cats \((y=0)\) and dogs \((y=1)\):&lt;/p&gt;

&lt;h4 id=&quot;approach-i&quot;&gt;&lt;strong&gt;Approach I:&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;Based on some features of an animal, given a training set, an algorithm like &lt;strong&gt;&lt;em&gt;logistic regression&lt;/em&gt;&lt;/strong&gt; or &lt;strong&gt;&lt;em&gt;perceptron&lt;/em&gt;&lt;/strong&gt; algorithm, tries to find a &lt;strong&gt;decision boundary&lt;/strong&gt; (e.g. a straight line) to separate the cats and the dogs.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;To classify a new animal:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Just check on which side of the decision boundary it falls.&lt;/p&gt;

&lt;h4 id=&quot;approach-ii&quot;&gt;&lt;strong&gt;Approach II:&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;First, look at cats, build a model of &lt;strong&gt;what cats look like&lt;/strong&gt;. Then, look at dogs, build another model of &lt;strong&gt;what dogs look like&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;To classify a new animal:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Match the new animal against the cat model and the dog model respectively, to see whether the new animal looks more like the cats or more like the dogs we had seen in the training set.&lt;/p&gt;

&lt;h3 id=&quot;12-definitions&quot;&gt;1.2. Definitions:&lt;/h3&gt;
&lt;h4 id=&quot;discriminative-learning-algorithms&quot;&gt;&lt;strong&gt;Discriminative&lt;/strong&gt; learning algorithms:&lt;/h4&gt;

&lt;p&gt;Algorithms that try to learn \(p(y \vert x)\) directly (such as logistic regression) or algorithms that try to learn mappings directly from the space of input \(\chi\) to the labels \({0,1}\) (such as perceptron algorithm) are called &lt;strong&gt;discriminative&lt;/strong&gt; learning algorithms.&lt;/p&gt;

&lt;h4 id=&quot;generative-learning-algorithms&quot;&gt;&lt;strong&gt;Generative&lt;/strong&gt; learning algorithms:&lt;/h4&gt;

&lt;p&gt;Algorithms that instead try to model \(p(x \vert y)\) and \(p(y)\) are called &lt;strong&gt;generative&lt;/strong&gt; learning algorithms.&lt;/p&gt;

&lt;h3 id=&quot;13-more-about-generative-learning-algorithms&quot;&gt;1.3. More about Generative Learning Algorithms:&lt;/h3&gt;

&lt;p&gt;Continue with the example of cats and dogs, if \(y\) indicates whether an sample is a cat (0) or a dog (1), then&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;\(p(x \vert y=0)\) models the distribution of cats’ features.&lt;/li&gt;
  &lt;li&gt;\(p(x \vert y=1)\) models the distribution of dogs’ features.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;After modeling \(p(y)\) (called the &lt;strong&gt;class priors&lt;/strong&gt;) and \(p(x \vert y)\), our algorithm can then use &lt;strong&gt;Bayes Rule&lt;/strong&gt; to derive the posterior distribution on \(y\) given \(x\):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(y \vert x) = \frac{p(x \vert y)p(y)}{p(x)}.&lt;/script&gt;

&lt;p&gt;Here, the denominator is given by \(p(x)=\sum_i p(x \vert y_i)p(y_i)\). In cats &amp;amp; dogs example, \(p(x)=p(x \vert y=0)p(y=0)+p(x \vert y=1)p(y=1)\).&lt;/p&gt;

&lt;p&gt;Actually, if we we’re calculating \(p(y \vert x)\) in order to make a prediction, then we don’t need to calculate the denominator, since&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\arg\underset{y}{\max} p(y \vert x) &amp; = \arg \underset{y}{\max} \frac{p(x \vert y)p(y)}{p(x)} \\
&amp; = \arg \underset{y}{\max} p(x \vert y)p(y).
\end{align} %]]&gt;&lt;/script&gt;

&lt;h2 id=&quot;2-gaussian-discriminant-analysis-gda&quot;&gt;2. Gaussian Discriminant Analysis (GDA)&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;(01/23/2019)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In this model (GDA), we assume that \(p(x \vert y)\) is distributed according to a &lt;strong&gt;multivariate normal distribution&lt;/strong&gt;. First let’s talk briefly about the properties of multivariate normal distributions.&lt;/p&gt;

&lt;h3 id=&quot;21-the-multivariate-normal-distribution&quot;&gt;2.1. The multivariate normal distribution&lt;/h3&gt;
&lt;p&gt;The Definition of so-called &lt;em&gt;multivariate normal distribution&lt;/em&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A distribution with a &lt;strong&gt;mean vector&lt;/strong&gt; \(\mu \in \mathbb{R}^n\) and a &lt;strong&gt;covariance matrix&lt;/strong&gt; \(\Sigma \in \mathbb{R}^{n \times n}\), where \(\Sigma \geq 0\) is symmetric and positive semi-definite. Also written “\(\mathcal{N}(\mu, \Sigma)\)”, it’s density is given by:&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x; \mu, \Sigma) = \frac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}}\exp\left(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)\right).&lt;/script&gt;

&lt;p&gt;where “\(\vert\Sigma\vert\)” denotes the determinant of the matrix \(\Sigma\).&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;For a random variable \(X\) distributed \(\mathcal{N}(\mu, \Sigma)\), the mean is given by \(\mu\):&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{E}[X] = \int_x x\ p(x;\mu,\Sigma)dx = \mu&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;The &lt;strong&gt;covariance&lt;/strong&gt; of a vector-valued random variable \(Z\) is defined as \(\text{Cov}(Z) = \text{E}[(Z-\text{E}[Z])(Z-\text{E}[Z])^T]\), which can also be written as \(\text{Cov}(Z) = \text{E}[ZZ^T]-(\text{E}[Z])(\text{E}[Z])^T\). If \(X \sim \mathcal{N}(\mu, \Sigma)\), then:&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{Cov}(X) = \Sigma.&lt;/script&gt;

&lt;h4 id=&quot;todo---practice&quot;&gt;//TODO - Practice:&lt;/h4&gt;

&lt;h3 id=&quot;22-the-gaussian-discriminant-analysis-model&quot;&gt;2.2. The Gaussian Discriminant Analysis model&lt;/h3&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://see.stanford.edu/materials/aimlcs229/cs229-notes2.pdf&quot;&gt;» Stanford CS229 Lecture Note Part IV - Generative Learning Algorithms&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;strong&gt;&lt;em&gt;KF&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;</content><author><name></name></author><category term="[&quot;Notes&quot;]" /><category term="machine learning" /><category term="cs229" /><summary type="html">A Review of Generative Learning Algorithms. Keep Updating: 2019-01-22 Add Part 1 2019-01-23 Add Part 2, Gausian discriminant analysis</summary></entry><entry><title type="html">How to Force Keras to use CPU to Run Script?</title><link href="http://localhost:4000/2019/01/23/how-to-force-keras-use-cpu" rel="alternate" type="text/html" title="How to Force Keras to use CPU to Run Script?" /><published>2019-01-23T14:00:00+08:00</published><updated>2019-01-23T14:00:00+08:00</updated><id>http://localhost:4000/2019/01/23/how-to-force-keras-use-cpu</id><content type="html" xml:base="http://localhost:4000/2019/01/23/how-to-force-keras-use-cpu">&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;The reason for such a demand:&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;My main training program was using the GPU fully. But I needed to get a prediction with another previously trained model urgently. I tried to use the GPU but I got OOM. Therefore, using CPU for the predicting job should be a good solution, and it did solve the problem!&lt;/p&gt;

  &lt;p&gt;Generally there are two ways: a short/lazy one and a lengthy but graceful one.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;option-i&quot;&gt;Option I:&lt;/h2&gt;
&lt;p&gt;If you want to force Keras to use CPU&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;os&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;environ&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;CUDA_DEVICE_ORDER&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;PCI_BUS_ID&quot;&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;environ&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;CUDA_VISIBLE_DEVICES&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;before Keras / Tensorflow is imported.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h2 id=&quot;option-ii&quot;&gt;Option II:&lt;/h2&gt;
&lt;p&gt;A rather graceful and separable way of doing this is to use&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;tensorflow&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;keras&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;backend&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;num_cores&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GPU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;num_GPU&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;num_CPU&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CPU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;num_CPU&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;num_GPU&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ConfigProto&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;intra_op_parallelism_threads&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_cores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;\
        &lt;span class=&quot;n&quot;&gt;inter_op_parallelism_threads&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_cores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;allow_soft_placement&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;\
        &lt;span class=&quot;n&quot;&gt;device_count&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'CPU'&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_CPU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'GPU'&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_GPU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;session&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Session&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_session&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;session&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Here with &lt;code class=&quot;highlighter-rouge&quot;&gt;booleans&lt;/code&gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;GPU&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;CPU&lt;/code&gt; you can specify whether to use a GPU or GPU when running your code.&lt;/p&gt;

&lt;p&gt;The only thing to note is that you’ll need &lt;code class=&quot;highlighter-rouge&quot;&gt;tensorflow-gpu&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;cuda/cudnn&lt;/code&gt; installed because you’re always giving the option of using a GPU.&lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://stackoverflow.com/questions/40690598/can-keras-with-tensorflow-backend-be-forced-to-use-cpu-or-gpu-at-will&quot;&gt;» Can Keras with Tensorflow backend be forced to use CPU or GPU at will? - Stack Overflow&lt;/a&gt;&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;strong&gt;&lt;em&gt;KF&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;</content><author><name></name></author><category term="[&quot;Deep Learning&quot;]" /><category term="how-to" /><category term="keras" /><category term="tensorflow" /><category term="deep learning" /><category term="gpu" /><category term="nvidia" /><category term="cuda" /><summary type="html">The reason for such a demand: My main training program was using the GPU fully. But I needed to get a prediction with another previously trained model urgently. I tried to use the GPU but I got OOM. Therefore, using CPU for the predicting job should be a good solution, and it did solve the problem! Generally there are two ways: a short/lazy one and a lengthy but graceful one. Option I: If you want to force Keras to use CPU import os os.environ[&quot;CUDA_DEVICE_ORDER&quot;] = &quot;PCI_BUS_ID&quot; os.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] = &quot;&quot; before Keras / Tensorflow is imported.</summary></entry><entry><title type="html">How to Add a Table of Content to my Jekyll Blog?</title><link href="http://localhost:4000/2019/01/23/how-to-add-table-of-content-in-jekyll" rel="alternate" type="text/html" title="How to Add a Table of Content to my Jekyll Blog?" /><published>2019-01-23T11:20:00+08:00</published><updated>2019-01-23T11:20:00+08:00</updated><id>http://localhost:4000/2019/01/23/how-to-add-table-of-content-in-jekyll</id><content type="html" xml:base="http://localhost:4000/2019/01/23/how-to-add-table-of-content-in-jekyll">&lt;p&gt;In the post, simple add:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;* TOC
{:toc} 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;in the post.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.seanbuscay.com/blog/jekyll-toc-markdown/&quot;&gt;» How I Add a Table of Contents to my Jekyll Blog Written in Markdown&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://blog.webjeda.com/jekyll-toc/#how-to-add-toc-for-jekyll-posts&quot;&gt;» Jekyll Table Of Contents - Wikipedia Look!&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;!--more--&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;strong&gt;&lt;em&gt;KF&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;</content><author><name></name></author><category term="[&quot;Web Developing&quot;]" /><category term="how-to" /><category term="jekyll" /><summary type="html">In the post, simple add: * TOC {:toc} in the post. » How I Add a Table of Contents to my Jekyll Blog Written in Markdown » Jekyll Table Of Contents - Wikipedia Look!</summary></entry><entry><title type="html">How to Add a Scroll Back to Top Button?</title><link href="http://localhost:4000/2019/01/23/how-to-add-back-to-top-button" rel="alternate" type="text/html" title="How to Add a Scroll Back to Top Button?" /><published>2019-01-23T09:40:00+08:00</published><updated>2019-01-23T09:40:00+08:00</updated><id>http://localhost:4000/2019/01/23/how-to-add-back-to-top-button</id><content type="html" xml:base="http://localhost:4000/2019/01/23/how-to-add-back-to-top-button">&lt;blockquote&gt;
  &lt;p&gt;Check this below:&lt;/p&gt;

  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&quot;https://www.w3schools.com/howto/howto_js_scroll_to_top.asp&quot;&gt;»How To Create a Scroll To Top Button - W3Schools&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;!--more--&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;strong&gt;&lt;em&gt;KF&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;</content><author><name></name></author><category term="[&quot;Web Developing&quot;]" /><category term="how-to" /><category term="jekyll" /><summary type="html">Check this below: »How To Create a Scroll To Top Button - W3Schools</summary></entry><entry><title type="html">How to Sort Site Tags in Jekyll?</title><link href="http://localhost:4000/2019/01/23/how-to-sort-tags-in-jekyll" rel="alternate" type="text/html" title="How to Sort Site Tags in Jekyll?" /><published>2019-01-23T09:20:00+08:00</published><updated>2019-01-23T09:20:00+08:00</updated><id>http://localhost:4000/2019/01/23/how-to-sort-tags-in-jekyll</id><content type="html" xml:base="http://localhost:4000/2019/01/23/how-to-sort-tags-in-jekyll">&lt;blockquote&gt;
  &lt;p&gt;Check this Article:&lt;/p&gt;

  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&quot;https://www.codeofclimber.ru/2015/sorting-site-tags-in-jekyll/&quot;&gt;» Sorting site tags in Jekyll - by “code of climber”&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;!--more--&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;strong&gt;&lt;em&gt;KF&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;</content><author><name></name></author><category term="[&quot;Web Developing&quot;]" /><category term="how-to" /><category term="jekyll" /><summary type="html">Check this Article: » Sorting site tags in Jekyll - by “code of climber”</summary></entry><entry><title type="html">How to Support LaTex in Jekyll?</title><link href="http://localhost:4000/2019/01/22/how-to-render-latex-in-jekyll" rel="alternate" type="text/html" title="How to Support LaTex in Jekyll?" /><published>2019-01-22T14:00:00+08:00</published><updated>2019-01-22T14:00:00+08:00</updated><id>http://localhost:4000/2019/01/22/how-to-render-latex-in-jekyll</id><content type="html" xml:base="http://localhost:4000/2019/01/22/how-to-render-latex-in-jekyll">&lt;blockquote&gt;
  &lt;p&gt;To render math symbols and formula using LaTex in Jekyll website, check the links below,&lt;/p&gt;

  &lt;p&gt;Also Remember to update the cdnjs link, which is described in the last reference link.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://stackoverflow.com/questions/10987992/using-mathjax-with-jekyll&quot;&gt;» mathjax-with-jekyll on Stack Overflow&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.iangoodfellow.com/blog/jekyll/markdown/tex/2016/11/07/latex-in-markdown.html&quot;&gt;» Iangoodfellow on LaTex in Jekyll&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.mathjax.org/cdn-shutting-down/&quot;&gt;» MathJax CDN shutting down on April 30, 2017. Alternatives available.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;!--more--&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;strong&gt;&lt;em&gt;KF&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;</content><author><name></name></author><category term="[&quot;Web Developing&quot;]" /><category term="how-to" /><category term="jekyll" /><category term="latex" /><summary type="html">To render math symbols and formula using LaTex in Jekyll website, check the links below, Also Remember to update the cdnjs link, which is described in the last reference link. Reference » mathjax-with-jekyll on Stack Overflow » Iangoodfellow on LaTex in Jekyll » MathJax CDN shutting down on April 30, 2017. Alternatives available.</summary></entry><entry><title type="html">What does ‘rc’ Mean in Unix-like systems, or even in Matplotlib rcParams?</title><link href="http://localhost:4000/2019/01/17/what-does-rc-mean" rel="alternate" type="text/html" title="What does 'rc' Mean in Unix-like systems, or even in Matplotlib rcParams?" /><published>2019-01-17T12:30:00+08:00</published><updated>2019-01-17T12:30:00+08:00</updated><id>http://localhost:4000/2019/01/17/what-does-rc-mean</id><content type="html" xml:base="http://localhost:4000/2019/01/17/what-does-rc-mean">&lt;blockquote&gt;
  &lt;p&gt;In the context of Unix-like systems, the term rc stands for the phrase &lt;strong&gt;“run commands”&lt;/strong&gt;. It is used for any file that contains startup information for a command. It is believed to have originated somewhere in 1965 from a runcom facility from the MIT Compatible Time-Sharing System (CTSS).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;From Brian Kernighan and Dennis Ritchie:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;There was a facility that would execute a bunch of commands stored in a file; it was called runcom for “run commands”, and the file began to be called “a runcom”. rc in Unix is a fossil from that usage.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!--more--&gt;

&lt;p&gt;Tom Van Vleck, a Multics engineer, has also reminisced about the extension rc: “The idea of having the command processing shell be an ordinary slave program came from the Multics design, and a predecessor program on CTSS by Louis Pouzin called RUNCOM, the source of the ‘.rc’ suffix on some Unix configuration files.”&lt;/p&gt;

&lt;p&gt;This is also the origin of the name of the Plan 9 from Bell Labs shell by Tom Duff, the rc shell. It is called “rc” because the main job of a shell is to “run commands”.&lt;/p&gt;

&lt;p&gt;While not historically precise, rc may also be expanded as “run control”, because an rc file controls how a program runs. For instance, the editor Vim looks for and reads the contents of the .vimrc file to determine its initial configuration. In The Art of Unix Programming, Eric S. Raymond consistently refers to rc files as “run-control” files.&lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Run_commands&quot;&gt;» Run commands - Wikipedia&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;strong&gt;&lt;em&gt;KF&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;</content><author><name></name></author><category term="[&quot;Python&quot;, &quot;Operating System&quot;]" /><category term="python" /><category term="unix" /><category term="linux" /><category term="matplotlib" /><summary type="html">In the context of Unix-like systems, the term rc stands for the phrase “run commands”. It is used for any file that contains startup information for a command. It is believed to have originated somewhere in 1965 from a runcom facility from the MIT Compatible Time-Sharing System (CTSS). From Brian Kernighan and Dennis Ritchie: There was a facility that would execute a bunch of commands stored in a file; it was called runcom for “run commands”, and the file began to be called “a runcom”. rc in Unix is a fossil from that usage.</summary></entry><entry><title type="html">How to Draw Contour Plot for Gradient Descent in Python</title><link href="http://localhost:4000/2019/01/17/gradient-descent-plot-contour" rel="alternate" type="text/html" title="How to Draw Contour Plot for Gradient Descent in Python" /><published>2019-01-17T09:10:00+08:00</published><updated>2019-01-17T09:10:00+08:00</updated><id>http://localhost:4000/2019/01/17/gradient-descent-plot-contour</id><content type="html" xml:base="http://localhost:4000/2019/01/17/gradient-descent-plot-contour">&lt;blockquote&gt;
  &lt;p&gt;The contour plot that showing the path of gradient descent often appears in the introductory part of machine learning. The following plot is an classic example from Andrew Ng’s CS229. In this article, I’d like to try and take a record on how to draw such a Gradient Descent contour plot in Python.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/public/img/20190117-contour_cs229.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;!--more--&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://www.adeveloperdiary.com/data-science/how-to-visualize-gradient-descent-using-contour-plot-in-python/&quot;&gt;» How to visualize Gradient Descent using Contour plot in Python - Abhisek Jana&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://matplotlib.org/tutorials/toolkits/mplot3d.html#toolkit-mplot3d-tutorial&quot;&gt;» The mplt3d Toolkit - matplotlib Official&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://matplotlib.org/gallery/index.html#mplot3d-examples-index&quot;&gt;» 3d Plotting examples - matplotlib Official&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;strong&gt;&lt;em&gt;KF&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;</content><author><name></name></author><category term="[&quot;Python&quot;]" /><category term="python" /><category term="how-to" /><category term="machine learning" /><summary type="html">The contour plot that showing the path of gradient descent often appears in the introductory part of machine learning. The following plot is an classic example from Andrew Ng’s CS229. In this article, I’d like to try and take a record on how to draw such a Gradient Descent contour plot in Python.</summary></entry></feed>